\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}R package}{2}{subsection.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Modeling}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Data Structure, Sampling Model, and Parameters}{2}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Method Structure and Clustering}{2}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}EBSeq}{2}{subsubsection.2.2.1}}
\citation{ref:dahl}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}modalClust}{4}{subsubsection.2.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Randomized $K-$means}{5}{subsubsection.2.2.3}}
\citation{selK}
\@writefile{lof}{\contentsline {figure}{\numberline {S1}{\ignorespaces Adjusted rand indexes to the clustering based on the original distance matrix without dividing weights. We investigate the randomness of clustering given by our weights through 8 datasets. All have stopping threshold for nlminb optimizing function in r with relative tolerance as 0.001\relax }}{6}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:ARI}{{S1}{6}{Adjusted rand indexes to the clustering based on the original distance matrix without dividing weights. We investigate the randomness of clustering given by our weights through 8 datasets. All have stopping threshold for nlminb optimizing function in r with relative tolerance as 0.001\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4}Selecting $K$}{6}{subsubsection.2.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {S2}{\ignorespaces comparison between random weighting scheme and bayesian clustering procedure in terms of posterior probabilities that two elements belong to the same class given the whole data and adjusted rand index comparing to the underlying true class label\relax }}{7}{figure.caption.2}}
\newlabel{fig:simu}{{S2}{7}{comparison between random weighting scheme and bayesian clustering procedure in terms of posterior probabilities that two elements belong to the same class given the whole data and adjusted rand index comparing to the underlying true class label\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Double Dirichlet Mixture}{8}{subsection.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Numerical Experiments}{10}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Synthetic Data}{10}{subsection.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {S3}{\ignorespaces first two principal components of transcripts under different parameters for simulated data. Different parameters resulted in different degree of separation of subtypes. We have 4 different settings for hyper-parameters of simulation, each setting has 2 replicates K = 3\relax }}{10}{figure.caption.3}}
\newlabel{fig:4}{{S3}{10}{first two principal components of transcripts under different parameters for simulated data. Different parameters resulted in different degree of separation of subtypes. We have 4 different settings for hyper-parameters of simulation, each setting has 2 replicates K = 3\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S4}{\ignorespaces K = 7\relax }}{11}{figure.caption.4}}
\newlabel{fig:5}{{S4}{11}{K = 7\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S5}{\ignorespaces K = 12\relax }}{11}{figure.caption.5}}
\newlabel{fig:6}{{S5}{11}{K = 12\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S6}{\ignorespaces Roc curve of the 12 simulation settings, under each setting, TPR and FPR are averaged over two replicates, generally we found scDDboost perform better than other methods\relax }}{12}{figure.caption.6}}
\newlabel{fig:7}{{S6}{12}{Roc curve of the 12 simulation settings, under each setting, TPR and FPR are averaged over two replicates, generally we found scDDboost perform better than other methods\relax }{figure.caption.6}{}}
\citation{Lane}
\citation{Shalek}
\citation{Trapnell}
\citation{Engel}
\citation{EMTAB}
\citation{Tasic}
\citation{oscope}
\citation{sc3}
\citation{Deng193}
\citation{Guo}
\citation{chu}
\citation{Darmanis}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Empirical Study}{13}{subsection.3.2}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces datasets used for comparisons of DD analysis under different methods\relax }}{13}{table.caption.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {S7}{\ignorespaces $P(ED_g|X,y)$ given by scDDboost versus empirical Wasserstein distance. Genes associated with boxes from left to right having $P(ED_g|X,y)$ range from 0 - 0.2, 0.2 - 0.4, 0.4 - 0.6, 0.6 - 0.8, 0.8 - 1, data used: FUCCI\relax }}{13}{figure.caption.8}}
\newlabel{fig:wad}{{S7}{13}{$P(ED_g|X,y)$ given by scDDboost versus empirical Wasserstein distance. Genes associated with boxes from left to right having $P(ED_g|X,y)$ range from 0 - 0.2, 0.2 - 0.4, 0.4 - 0.6, 0.6 - 0.8, 0.8 - 1, data used: FUCCI\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S8}{\ignorespaces $P(ED_g|X,y)$ given by scDDboost versus empirical Wasserstein distance. Genes associated with boxes from left to right having $P(ED_g|X,y)$ range from 0 - 0.2, 0.2 - 0.4, 0.4 - 0.6, 0.6 - 0.8, 0.8 - 1, similar plots as supplementary Fig 7, now is for the simulation cases\relax }}{14}{figure.caption.9}}
\newlabel{fig:8}{{S8}{14}{$P(ED_g|X,y)$ given by scDDboost versus empirical Wasserstein distance. Genes associated with boxes from left to right having $P(ED_g|X,y)$ range from 0 - 0.2, 0.2 - 0.4, 0.4 - 0.6, 0.6 - 0.8, 0.8 - 1, similar plots as supplementary Fig 7, now is for the simulation cases\relax }{figure.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces datasets used for null cases, as cells are coming from same biological condition, there should not be any differential distributed genes, any positive call is false positive\relax }}{14}{table.caption.10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Robustness}{15}{subsection.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {S9}{\ignorespaces PDD change under different number of subtypes $K$, dataset used DEC-EC, our rule for selecting $K$ tends also to make PDD stabilize\relax }}{15}{figure.caption.11}}
\newlabel{fig:rwk}{{S9}{15}{PDD change under different number of subtypes $K$, dataset used DEC-EC, our rule for selecting $K$ tends also to make PDD stabilize\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S10}{\ignorespaces DEC-EC, PDD under K = 5 vs. K = 6, left panel is without the randomized distance and right panel is with randomized distance. We increase robustness of our methods through random weighting\relax }}{15}{figure.caption.12}}
\@writefile{lof}{\contentsline {figure}{\numberline {S11}{\ignorespaces under NULL case, using dataset EMTAB2805, when using too big $K$ we may lose FDR control (black dashed line shows proportion of false positive identified by scDDboost under 0.05 threshold, while validity score did not vary too much after $K$ is greater than 2 \relax }}{16}{figure.caption.13}}
\newlabel{fig:lfdr}{{S11}{16}{under NULL case, using dataset EMTAB2805, when using too big $K$ we may lose FDR control (black dashed line shows proportion of false positive identified by scDDboost under 0.05 threshold, while validity score did not vary too much after $K$ is greater than 2 \relax }{figure.caption.13}{}}
\citation{ineq}
\@writefile{toc}{\contentsline {section}{\numberline {4}Posterior consistency}{17}{section.4}}
\bibstyle{imsart-nameyear}
\bibdata{./supp_references/wlr_ref}
\bibcite{EMTAB}{{1}{2015}{{Buettner et~al.}}{{}}}
\bibcite{chu}{{2}{2016}{{Chu et~al.}}{{}}}
\bibcite{ref:dahl}{{3}{2009}{{Dahl}}{{}}}
\bibcite{Darmanis}{{4}{2017}{{Darmanis et~al.}}{{}}}
\bibcite{Deng193}{{5}{2014}{{Deng et~al.}}{{}}}
\bibcite{Engel}{{6}{2016}{{Engel et~al.}}{{}}}
\bibcite{Guo}{{7}{2015}{{Guo et~al.}}{{}}}
\bibcite{sc3}{{8}{2017}{{Kiselev et~al.}}{{}}}
\bibcite{Lane}{{9}{2017}{{Lane et~al.}}{{}}}
\@writefile{toc}{\contentsline {section}{References}{19}{section*.15}}
\bibcite{oscope}{{10}{2015}{{Leng et~al.}}{{}}}
\bibcite{ineq}{{11}{2007}{{Li and ping Chen}}{{}}}
\bibcite{selK}{{12}{2000}{{Ray and Turi}}{{}}}
\bibcite{Shalek}{{13}{2014}{{Shalek et~al.}}{{}}}
\bibcite{Tasic}{{14}{2016}{{Tasic et~al.}}{{}}}
\bibcite{Trapnell}{{15}{2014}{{Trapnell et~al.}}{{}}}
