\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}DEC vs EC}{2}{subsection.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Potentially to be DD genes uniquely identified by us. \relax }}{2}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:4}{{1}{2}{Potentially to be DD genes uniquely identified by us. \relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}R package}{2}{subsection.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Modeling}{3}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Data Structure, Sampling Model, and Parameters}{3}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Method Structure and Clustering}{3}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}EBSeq}{3}{subsubsection.2.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}modalClust}{4}{subsubsection.2.2.2}}
\citation{ref:dahl}
\citation{selK}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Randomized $K-$means}{6}{subsubsection.2.2.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4}Selecting $K$}{6}{subsubsection.2.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Double Dirichlet Mixture}{7}{subsection.2.3}}
\citation{Lane}
\citation{Shalek}
\citation{Trapnell}
\citation{Engel}
\citation{EMTAB}
\citation{Tasic}
\citation{oscope}
\citation{sc3}
\citation{Deng193}
\citation{Guo}
\citation{chu}
\citation{Darmanis}
\@writefile{toc}{\contentsline {section}{\numberline {3}Numerical Experiments}{9}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Synthetic Data}{9}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Empirical Study}{9}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Null cases}{9}{subsection.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces first two principal components of transcripts under different parameters for simulated data. Different parameters resulted in different degree of separation of subtypes. We have 4 different settings for hyper-parameters of simulation, each setting has 2 replicates K = 3\relax }}{10}{figure.caption.2}}
\newlabel{fig:4}{{2}{10}{first two principal components of transcripts under different parameters for simulated data. Different parameters resulted in different degree of separation of subtypes. We have 4 different settings for hyper-parameters of simulation, each setting has 2 replicates K = 3\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces similar plots K = 7\relax }}{10}{figure.caption.3}}
\newlabel{fig:5}{{3}{10}{similar plots K = 7\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces similar plots K = 12\relax }}{11}{figure.caption.4}}
\newlabel{fig:6}{{4}{11}{similar plots K = 12\relax }{figure.caption.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces datasets used for comparisons of DD analysis under different methods\relax }}{11}{table.caption.6}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Random weighting and proof for consistency theorems}{11}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Stability of posterior under random weighting}{11}{subsection.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Roc curve of the 12 simulation settings, under each setting, TPR and FPR are averaged over two replicates, generally we found scDDboost perform better than other methods\relax }}{12}{figure.caption.5}}
\newlabel{fig:7}{{5}{12}{Roc curve of the 12 simulation settings, under each setting, TPR and FPR are averaged over two replicates, generally we found scDDboost perform better than other methods\relax }{figure.caption.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces datasets used for null cases, as cells are coming from same biological condition, there should not be any differential distributed genes, any positive call is false positive\relax }}{12}{table.caption.8}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces comparison of posterior probabilities of being DD among different number of subtypes, when we underestimate the number of subtypes, the difference is huge, see PDD between 6 subtypes and 7 subtypes. There is an approximate horizontal line with massive points at the top of left panel, which indicate that we underestimate lots of DD genes due to underestimate the number of subtypes. While in the case when we overestimate the number of subtypes 7 subtypes vs. 8 subtypes, though inflating PDD but the variation of difference is small, from 6 to 8 subtypes the PDD become more linear related.\relax }}{13}{figure.caption.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces under NULL case, using dataset EMTAB2805, when using too big $K$ we may lose FDR control (black dashed line shows proportion of false positive identified by scDDboost under 0.05 threshold, while validity score did not vary too much after $K$ is greater than 2 \relax }}{14}{figure.caption.9}}
\newlabel{fig:7}{{7}{14}{under NULL case, using dataset EMTAB2805, when using too big $K$ we may lose FDR control (black dashed line shows proportion of false positive identified by scDDboost under 0.05 threshold, while validity score did not vary too much after $K$ is greater than 2 \relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces selecting number of subtypes for data GSE57872, we observe posterior probabilities become stable at more than 6 subtypes. Since increasing number of subtypes tends to decrease sample size of each subtypes, make complicate constraints for equivalent distribution and inflate estimated PDD. We select number of subtypes to be 7\relax }}{14}{figure.caption.10}}
\citation{ref:d3e}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces selecting number of subtypes for data GSE48968, we observe posterior probabilities become stable at more than 5 subtypes\relax }}{15}{figure.caption.11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Bursting parameters}{15}{subsection.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces D3E method will estimate 3 bursting parameters probability of a gene being on (\textbf  {a}) and off (\textbf  {b}) and the expression rate when the gene expression is on (\textbf  {c}), we plot the hexbin plot of probability of a gene being DD under out method v.s. the absolute value of log fold change of \textbf  {a} , \textbf  {b} and \textbf  {c} across the two conditions accordingly. The log fold change is scaled by dividing the largest log fold change so that ends up in a value between 0 and 1 Here we use the GSE71585 data \relax }}{15}{figure.caption.12}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces 2D histogram for bursting parameters of DD genes identified by scDDboost from dataset EMTAB2805 estimated by D3E. Left panel : comparison of rate of promoter activation between two conditions, similarly, middle panel : rate of promoter inactivation and right panel: rate of transcription when the promoter is in the active state. We observe that difference between transcription rate is smaller compare to difference between the activation and inactivation rate.\relax }}{16}{figure.caption.13}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Theoretical issues}{16}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Posterior consistency}{16}{subsection.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Four subtypes of cells, simplexes of $(\phi ,\psi )$ satisfying different constraints.\relax }}{17}{figure.caption.14}}
\newlabel{fig:1}{{12}{17}{Four subtypes of cells, simplexes of $(\phi ,\psi )$ satisfying different constraints.\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Random weighting}{18}{subsection.5.2}}
\citation{ineq}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}simulation}{19}{subsection.5.3}}
\bibstyle{imsart-nameyear}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces comparison between random weighting scheme and bayesian clustering procedure in terms of posterior probabilities that two elements belong to the same class given the whole data and adjusted rand index comparing to the underlying true class label\relax }}{23}{figure.caption.15}}
\newlabel{fig:1}{{13}{23}{comparison between random weighting scheme and bayesian clustering procedure in terms of posterior probabilities that two elements belong to the same class given the whole data and adjusted rand index comparing to the underlying true class label\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Adjusted rand indexes to the clustering based on the original distance matrix without dividing weights. We investigate the randomness of clustering given by our weights through 8 datasets. All have stopping threshold for nlminb optimizing function in r with relative tolerance as 0.01\relax }}{24}{figure.caption.16}}
\newlabel{fig:1}{{14}{24}{Adjusted rand indexes to the clustering based on the original distance matrix without dividing weights. We investigate the randomness of clustering given by our weights through 8 datasets. All have stopping threshold for nlminb optimizing function in r with relative tolerance as 0.01\relax }{figure.caption.16}{}}
